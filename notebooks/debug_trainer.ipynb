{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Trainer\n",
    "Mirror of the complex U-Net run from `experiment_runner.ipynb` with hooks to inspect losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_ENABLE_MPS_FALLBACK=1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "%env PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "if PROJECT_ROOT.name == 'notebooks':\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "SRC_ROOT = PROJECT_ROOT / 'src'\n",
    "if str(SRC_ROOT) not in sys.path:\n",
    "    sys.path.append(str(SRC_ROOT))\n",
    "\n",
    "from data.dataset import SingleCoilDataset\n",
    "from data.masking import EquispacedMasker\n",
    "from models.real_unet import RealUnet\n",
    "from models.cx_unet import ComplexUnet\n",
    "from training.utils import test_loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'complex',\n",
       " 'train_folder': '/Users/giodegeronimo/Desktop/ECE570/ece570-tinyreproductions/data/singlecoil_train',\n",
       " 'val_folder': '/Users/giodegeronimo/Desktop/ECE570/ece570-tinyreproductions/data/singlecoil_val',\n",
       " 'mask': {'accel': 4, 'acs': 24},\n",
       " 'train_subset': 1024,\n",
       " 'val_subset': 256,\n",
       " 'batch_size': 4,\n",
       " 'num_workers': 2,\n",
       " 'epochs': 15,\n",
       " 'learning_rate': 0.0001,\n",
       " 'features_real': [32, 64, 128, 256, 512],\n",
       " 'features_complex': [32, 64, 128, 256, 512],\n",
       " 'width_scale': 1.416,\n",
       " 'seed': 1,\n",
       " 'log_every': 10}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "CONFIG = {\n",
    "    'model': 'complex',  # 'complex' or 'real'\n",
    "    'train_folder': str(PROJECT_ROOT / 'data' / 'singlecoil_train'),\n",
    "    'val_folder': str(PROJECT_ROOT / 'data' / 'singlecoil_val'),\n",
    "    'mask': {'accel': 4, 'acs': 24},\n",
    "    'train_subset': 1024,\n",
    "    'val_subset': 256,\n",
    "    'batch_size': 4,\n",
    "    'num_workers': 2,\n",
    "    'epochs': 15,\n",
    "    'learning_rate': 1e-4,\n",
    "    'features_real': [32, 64, 128, 256, 512],\n",
    "    'features_complex': [32, 64, 128, 256, 512],\n",
    "    'width_scale': 1.416,\n",
    "    'seed': 1,\n",
    "    'log_every': 10,\n",
    "}\n",
    "CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 256)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "masker = EquispacedMasker(accel=CONFIG['mask']['accel'], acs=CONFIG['mask']['acs'])\n",
    "train_full = SingleCoilDataset(CONFIG['train_folder'], mask_func=masker)\n",
    "val_full = SingleCoilDataset(CONFIG['val_folder'], mask_func=masker)\n",
    "train_indices = list(range(min(CONFIG['train_subset'], len(train_full))))\n",
    "val_indices = list(range(min(CONFIG['val_subset'], len(val_full))))\n",
    "train_set = Subset(train_full, train_indices)\n",
    "val_set = Subset(val_full, val_indices)\n",
    "train_loader = DataLoader(train_set, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=CONFIG['num_workers'])\n",
    "val_loader = DataLoader(val_set, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=CONFIG['num_workers'])\n",
    "len(train_set), len(val_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(CONFIG['seed'])\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if CONFIG['model'] == 'complex':\n",
    "    model = ComplexUnet(in_channels=1, out_channels=1, features=CONFIG['features_complex']).to(device)\n",
    "else:\n",
    "    model = RealUnet(in_channels=1, out_channels=1, features=CONFIG['features_real'], width_scale=CONFIG['width_scale']).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
    "loss_fn = lambda pred, target: (pred - target).abs().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils as nn_utils\n",
    "\n",
    "\n",
    "\n",
    "def train_epoch_debug(model, dataloader, optimizer, loss_fn, device, log_every=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for step, (masked, target) in enumerate(dataloader, start=1):\n",
    "        masked = masked.to(device)\n",
    "        target = target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(masked)\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        from pathlib import Path\n",
    "\n",
    "        def safe_to_numpy(x):\n",
    "            return x.detach().cpu().numpy()\n",
    "\n",
    "        DEBUG_SAVE_DIR = Path(\"debug_outputs\")\n",
    "        DEBUG_SAVE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "        if step % 20 == 0:\n",
    "            idx = 0  # pick first item in batch\n",
    "            gt = target[idx].abs()\n",
    "            pd = pred[idx].abs()\n",
    "            gt_np = safe_to_numpy(gt.squeeze())\n",
    "            pd_np = safe_to_numpy(pd.squeeze())\n",
    "\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
    "            axes[0].imshow(gt_np, cmap=\"gray\")\n",
    "            axes[0].set_title(\"Target\")\n",
    "            axes[0].axis(\"off\")\n",
    "            axes[1].imshow(pd_np, cmap=\"gray\")\n",
    "            axes[1].set_title(\"Prediction\")\n",
    "            axes[1].axis(\"off\")\n",
    "            fig.tight_layout()\n",
    "\n",
    "            out_path = DEBUG_SAVE_DIR / f\"step{step:04d}.png\"\n",
    "            fig.savefig(out_path, dpi=200)\n",
    "            plt.close(fig)\n",
    "\n",
    "        loss = loss_fn(pred, target)\n",
    "        print(loss.item())\n",
    "        loss.backward()\n",
    "        #print(min(param.grad.min().item() for param in model.parameters()), max(param.grad.max().item() for param in model.parameters()))\n",
    "        #nn_utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        #print(min(param.grad.min().item() for param in model.parameters()), max(param.grad.max().item() for param in model.parameters()))\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()            \n",
    "        if log_every is not None and (step % log_every == 0 or step == len(dataloader)):\n",
    "            print(f\"Step {step}/{len(dataloader)} loss {loss.item():.4e}\")\n",
    "    return total_loss / max(len(dataloader), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giodegeronimo/Desktop/ECE570/ece570-tinyreproductions/src/models/cx_unet.py:143: UserWarning: The operator 'aten::_linalg_eigh.eigenvalues' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:15.)\n",
      "  eigvals, eigvecs = torch.linalg.eigh(V)  # (C, 2), (C, 2, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5534026026725769\n",
      "0.5131915211677551\n",
      "0.5409350991249084\n",
      "0.4946202337741852\n",
      "0.44553518295288086\n",
      "0.43648549914360046\n",
      "0.43758314847946167\n",
      "0.4134536683559418\n",
      "0.41041579842567444\n",
      "0.37925222516059875\n",
      "Step 10/256 loss 3.7925e-01\n",
      "0.379495769739151\n",
      "0.3685612380504608\n",
      "0.3839791715145111\n",
      "0.3524315059185028\n",
      "0.3307341933250427\n",
      "0.34352073073387146\n",
      "0.35040533542633057\n",
      "0.3063223958015442\n",
      "0.33852720260620117\n",
      "0.31633952260017395\n",
      "Step 20/256 loss 3.1634e-01\n",
      "0.3091432750225067\n",
      "0.309565931558609\n",
      "0.30736008286476135\n",
      "0.2835833430290222\n",
      "0.2834860682487488\n",
      "0.2694880962371826\n",
      "0.301587849855423\n",
      "0.26829975843429565\n",
      "0.28632667660713196\n",
      "0.25363367795944214\n",
      "Step 30/256 loss 2.5363e-01\n",
      "0.25624194741249084\n",
      "0.264178991317749\n",
      "0.2338613122701645\n",
      "0.29253244400024414\n",
      "0.22331306338310242\n",
      "0.24628959596157074\n",
      "0.25774380564689636\n",
      "0.2107411026954651\n",
      "0.23133087158203125\n",
      "0.23793725669384003\n",
      "Step 40/256 loss 2.3794e-01\n",
      "0.24886386096477509\n",
      "0.24104513227939606\n",
      "0.23012231290340424\n",
      "0.2552434206008911\n",
      "0.258008748292923\n",
      "0.2502957880496979\n",
      "0.24333374202251434\n",
      "0.2660183906555176\n",
      "0.2524482309818268\n",
      "0.25471529364585876\n",
      "Step 50/256 loss 2.5472e-01\n",
      "0.24318808317184448\n",
      "0.254354864358902\n",
      "0.28360313177108765\n",
      "0.26543179154396057\n",
      "0.2638201415538788\n",
      "0.2877599596977234\n",
      "0.27099141478538513\n",
      "0.30173811316490173\n",
      "0.27391937375068665\n",
      "0.2946200370788574\n",
      "Step 60/256 loss 2.9462e-01\n",
      "0.2906225025653839\n",
      "0.29463234543800354\n",
      "0.28175413608551025\n",
      "0.2906491458415985\n",
      "0.31605684757232666\n",
      "0.3024616539478302\n",
      "0.3319641053676605\n",
      "0.29393884539604187\n",
      "0.3239482045173645\n",
      "0.29550787806510925\n",
      "Step 70/256 loss 2.9551e-01\n",
      "0.374022513628006\n",
      "0.3266766369342804\n",
      "0.3431587219238281\n",
      "0.311313658952713\n",
      "0.34942612051963806\n",
      "0.33120420575141907\n",
      "0.3275167644023895\n",
      "0.31986066699028015\n",
      "0.3287835717201233\n",
      "0.33754828572273254\n",
      "Step 80/256 loss 3.3755e-01\n",
      "0.3005274832248688\n",
      "0.35254836082458496\n",
      "0.3247413635253906\n",
      "0.3510988652706146\n",
      "0.33009767532348633\n",
      "0.33071577548980713\n",
      "0.33421221375465393\n",
      "0.32289862632751465\n",
      "0.3345150351524353\n",
      "0.31104761362075806\n",
      "Step 90/256 loss 3.1105e-01\n",
      "0.3567064106464386\n",
      "0.2926749289035797\n",
      "0.3079550564289093\n",
      "0.3178199231624603\n",
      "0.30890870094299316\n",
      "0.31051477789878845\n",
      "0.31834864616394043\n",
      "0.2962557077407837\n",
      "0.302101194858551\n",
      "0.3094829022884369\n",
      "Step 100/256 loss 3.0948e-01\n",
      "0.293018639087677\n",
      "0.33208826184272766\n",
      "0.3012537956237793\n",
      "0.30813664197921753\n",
      "0.30936935544013977\n",
      "0.3025497496128082\n",
      "0.3225241005420685\n",
      "0.32096555829048157\n",
      "0.3102050721645355\n",
      "0.316886842250824\n",
      "Step 110/256 loss 3.1689e-01\n",
      "0.3128506541252136\n",
      "0.3211619555950165\n",
      "0.30210351943969727\n",
      "0.3178410828113556\n",
      "0.31424397230148315\n",
      "0.3399052619934082\n",
      "0.37185776233673096\n",
      "0.3348924219608307\n",
      "0.352014422416687\n",
      "0.376758337020874\n",
      "Step 120/256 loss 3.7676e-01\n",
      "0.33019888401031494\n",
      "0.32562771439552307\n",
      "0.38084277510643005\n",
      "0.34214088320732117\n",
      "0.3544273376464844\n",
      "0.40292686223983765\n",
      "0.3662496507167816\n",
      "0.3702658414840698\n",
      "0.34319648146629333\n",
      "0.3335883319377899\n",
      "Step 130/256 loss 3.3359e-01\n",
      "0.34065207839012146\n",
      "0.3887474238872528\n",
      "0.3582100570201874\n",
      "0.3585968613624573\n",
      "0.3577992022037506\n",
      "0.3268272578716278\n",
      "0.33794260025024414\n",
      "0.36269044876098633\n",
      "0.36554571986198425\n",
      "0.38555246591567993\n",
      "Step 140/256 loss 3.8555e-01\n",
      "0.361613392829895\n",
      "0.3509254455566406\n",
      "0.37395960092544556\n",
      "0.3907468318939209\n",
      "0.3318733274936676\n",
      "0.36458808183670044\n",
      "0.32585322856903076\n",
      "0.36356642842292786\n",
      "0.3686259090900421\n",
      "0.35814589262008667\n",
      "Step 150/256 loss 3.5815e-01\n",
      "0.3700365424156189\n",
      "0.3510931432247162\n",
      "0.31000685691833496\n",
      "0.3704589903354645\n",
      "0.3297363519668579\n",
      "0.3487277925014496\n",
      "0.37401655316352844\n",
      "0.3463992178440094\n",
      "0.34891510009765625\n",
      "0.3403763473033905\n",
      "Step 160/256 loss 3.4038e-01\n",
      "0.34624090790748596\n",
      "0.33625850081443787\n",
      "0.3069235682487488\n",
      "0.3261898458003998\n",
      "0.3449592590332031\n",
      "0.35672271251678467\n",
      "0.34218841791152954\n",
      "0.32569724321365356\n",
      "0.317309707403183\n",
      "0.34237194061279297\n",
      "Step 170/256 loss 3.4237e-01\n",
      "0.32626986503601074\n",
      "0.3890518248081207\n",
      "0.3445366621017456\n",
      "0.34061720967292786\n",
      "0.3519692122936249\n",
      "0.37449026107788086\n",
      "0.35053014755249023\n",
      "0.3643834590911865\n",
      "0.3515302240848541\n",
      "0.3301036059856415\n",
      "Step 180/256 loss 3.3010e-01\n",
      "0.35318049788475037\n",
      "0.3626641631126404\n",
      "0.35864004492759705\n",
      "0.31333625316619873\n",
      "0.35978278517723083\n",
      "0.33166900277137756\n",
      "0.36232173442840576\n",
      "0.34805336594581604\n",
      "0.3440963625907898\n",
      "0.45383644104003906\n",
      "Step 190/256 loss 4.5384e-01\n",
      "0.3654266893863678\n",
      "0.3362062871456146\n",
      "0.35185927152633667\n",
      "0.35717275738716125\n",
      "0.3581635653972626\n",
      "0.3365064859390259\n",
      "0.37891700863838196\n",
      "0.3671697974205017\n",
      "0.371764600276947\n",
      "0.39608994126319885\n",
      "Step 200/256 loss 3.9609e-01\n",
      "0.3844106197357178\n",
      "0.37236693501472473\n",
      "0.38916468620300293\n",
      "0.40060949325561523\n",
      "0.41644972562789917\n",
      "0.39039748907089233\n",
      "0.42056146264076233\n",
      "0.378435879945755\n",
      "0.43421462178230286\n",
      "0.43560823798179626\n",
      "Step 210/256 loss 4.3561e-01\n",
      "0.3646511137485504\n",
      "0.3783056437969208\n",
      "0.4139224588871002\n",
      "0.420247346162796\n",
      "0.4233388900756836\n",
      "0.4309777319431305\n",
      "0.47806960344314575\n",
      "0.4450008273124695\n",
      "0.4816415011882782\n",
      "0.4595138430595398\n",
      "Step 220/256 loss 4.5951e-01\n",
      "0.44102942943573\n",
      "0.4548517167568207\n",
      "0.47636088728904724\n",
      "0.44568225741386414\n",
      "0.4481970965862274\n",
      "0.45359006524086\n",
      "0.47827354073524475\n",
      "0.43433910608291626\n",
      "0.42314857244491577\n",
      "0.45756563544273376\n",
      "Step 230/256 loss 4.5757e-01\n",
      "0.46191200613975525\n",
      "0.4756000339984894\n",
      "0.45233985781669617\n",
      "0.4892541468143463\n",
      "0.46676963567733765\n",
      "0.5282170176506042\n",
      "0.4732683598995209\n",
      "0.45765531063079834\n",
      "0.435647577047348\n",
      "0.4631190896034241\n",
      "Step 240/256 loss 4.6312e-01\n",
      "0.4357948303222656\n",
      "0.45136550068855286\n",
      "0.48768237233161926\n",
      "0.46491220593452454\n",
      "0.48908889293670654\n",
      "0.49155279994010925\n",
      "0.4744313657283783\n",
      "0.4620760977268219\n",
      "0.4932050108909607\n",
      "0.5044130682945251\n",
      "Step 250/256 loss 5.0441e-01\n",
      "0.5465570688247681\n",
      "0.5019954442977905\n",
      "0.5384783148765564\n",
      "0.46897754073143005\n",
      "0.5253570079803467\n",
      "0.5165393948554993\n",
      "Step 256/256 loss 5.1654e-01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf1866a14765450fba128acb3e57fe0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | train 3.5774e-01 | val 5.8970e-01\n",
      "0.5723958611488342\n",
      "0.4838281273841858\n",
      "0.5285736918449402\n",
      "0.47784942388534546\n",
      "0.5463464856147766\n",
      "0.5428199172019958\n",
      "0.5108019113540649\n",
      "0.5383570194244385\n",
      "0.5637804865837097\n",
      "0.5937706828117371\n",
      "Step 10/256 loss 5.9377e-01\n",
      "0.5514652729034424\n",
      "0.5421373844146729\n",
      "0.5517671704292297\n",
      "0.5477685332298279\n",
      "0.5033969879150391\n",
      "0.4834699034690857\n",
      "0.5044769048690796\n",
      "0.5057443976402283\n",
      "0.46205398440361023\n",
      "0.47638121247291565\n",
      "Step 20/256 loss 4.7638e-01\n",
      "0.5393104553222656\n",
      "0.5735322833061218\n",
      "0.4287916123867035\n",
      "0.5033366084098816\n",
      "0.469308465719223\n",
      "0.4665353298187256\n",
      "0.47525084018707275\n",
      "0.46237269043922424\n",
      "0.4551950991153717\n",
      "0.5216749310493469\n",
      "Step 30/256 loss 5.2167e-01\n",
      "0.4087998867034912\n",
      "0.47094568610191345\n",
      "0.4378039538860321\n",
      "0.451725035905838\n",
      "0.4483736455440521\n",
      "0.4833648204803467\n",
      "0.5121293663978577\n",
      "0.4464607238769531\n",
      "0.4628717005252838\n",
      "0.43592140078544617\n",
      "Step 40/256 loss 4.3592e-01\n",
      "0.4326455593109131\n",
      "0.524563193321228\n",
      "0.498033732175827\n",
      "0.44355064630508423\n",
      "0.5343284010887146\n",
      "0.49623721837997437\n",
      "0.5509721040725708\n",
      "0.46509242057800293\n",
      "0.4935987889766693\n",
      "0.45859548449516296\n",
      "Step 50/256 loss 4.5860e-01\n",
      "0.4622933864593506\n",
      "0.5130444169044495\n",
      "0.48997053503990173\n",
      "0.5178341269493103\n",
      "0.44007232785224915\n",
      "0.46352729201316833\n",
      "0.4971596896648407\n",
      "0.4235030710697174\n",
      "0.5432209372520447\n",
      "0.47589606046676636\n",
      "Step 60/256 loss 4.7590e-01\n",
      "0.5205641984939575\n",
      "0.44075027108192444\n",
      "0.5350486636161804\n",
      "0.4915531873703003\n",
      "0.5645031332969666\n",
      "0.5100578665733337\n",
      "0.5236993432044983\n",
      "0.4672106206417084\n",
      "0.512966513633728\n",
      "0.4861890375614166\n",
      "Step 70/256 loss 4.8619e-01\n",
      "0.5391051173210144\n",
      "0.4632684886455536\n",
      "0.49531683325767517\n",
      "0.47499674558639526\n",
      "0.43107762932777405\n",
      "0.5065180659294128\n",
      "0.4884244501590729\n",
      "0.5020225644111633\n",
      "0.5416406989097595\n",
      "0.46765604615211487\n",
      "Step 80/256 loss 4.6766e-01\n",
      "0.47775253653526306\n",
      "0.5403264760971069\n",
      "0.552736222743988\n",
      "0.5051388144493103\n",
      "0.5310139656066895\n",
      "0.4874579608440399\n",
      "0.5492388606071472\n",
      "0.5329341888427734\n",
      "0.5454801917076111\n",
      "0.4528416693210602\n",
      "Step 90/256 loss 4.5284e-01\n",
      "0.5104138255119324\n",
      "0.4950362741947174\n",
      "0.515625536441803\n",
      "0.4770856499671936\n",
      "0.5229815244674683\n",
      "0.502649188041687\n",
      "0.4821089804172516\n",
      "0.5151349306106567\n",
      "0.48087233304977417\n",
      "0.4950699508190155\n",
      "Step 100/256 loss 4.9507e-01\n",
      "0.463532030582428\n",
      "0.47180885076522827\n",
      "0.499508261680603\n",
      "0.46034860610961914\n",
      "0.47794920206069946\n",
      "0.44785040616989136\n",
      "0.43432772159576416\n",
      "0.4525669813156128\n",
      "0.48500674962997437\n",
      "0.4416167438030243\n",
      "Step 110/256 loss 4.4162e-01\n",
      "0.5188543796539307\n",
      "0.4818090796470642\n",
      "0.4769686162471771\n",
      "0.4375408887863159\n",
      "0.47264569997787476\n",
      "0.43282243609428406\n",
      "0.4791632294654846\n",
      "0.44249945878982544\n",
      "0.4713980257511139\n",
      "0.464288592338562\n",
      "Step 120/256 loss 4.6429e-01\n",
      "0.48184293508529663\n",
      "0.48128563165664673\n",
      "0.4587232768535614\n",
      "0.44126540422439575\n",
      "0.4141734838485718\n",
      "0.4604947566986084\n",
      "0.4325500428676605\n",
      "0.4226044714450836\n",
      "0.492145836353302\n",
      "0.42250537872314453\n",
      "Step 130/256 loss 4.2251e-01\n",
      "0.447390615940094\n",
      "0.4876145124435425\n",
      "0.44277843832969666\n",
      "0.4568929076194763\n",
      "0.4525302052497864\n",
      "0.3848221004009247\n",
      "0.41479700803756714\n",
      "0.42470863461494446\n",
      "0.4401835501194\n",
      "0.39873403310775757\n",
      "Step 140/256 loss 3.9873e-01\n",
      "0.3884789049625397\n",
      "0.41276076436042786\n",
      "0.3733290731906891\n",
      "0.35798752307891846\n",
      "0.38383322954177856\n",
      "0.3901916742324829\n",
      "0.3823354244232178\n",
      "0.4164292812347412\n",
      "0.3725505769252777\n",
      "0.355666846036911\n",
      "Step 150/256 loss 3.5567e-01\n",
      "0.3597467839717865\n",
      "0.3927849233150482\n",
      "0.36297935247421265\n",
      "0.3441866934299469\n",
      "0.4070829749107361\n",
      "0.361523300409317\n",
      "0.3430841863155365\n",
      "0.348061203956604\n",
      "0.3740766942501068\n",
      "0.3395983576774597\n",
      "Step 160/256 loss 3.3960e-01\n",
      "0.3637019395828247\n",
      "0.3517196476459503\n",
      "0.3474058508872986\n",
      "0.37334784865379333\n",
      "0.36605018377304077\n",
      "0.3455129861831665\n",
      "0.37151703238487244\n",
      "0.3371717631816864\n",
      "0.34016212821006775\n",
      "0.3511422574520111\n",
      "Step 170/256 loss 3.5114e-01\n",
      "0.3789748251438141\n",
      "0.3537788987159729\n",
      "0.3517796993255615\n",
      "0.3101465106010437\n",
      "0.3036796450614929\n",
      "0.35592472553253174\n",
      "0.3404725193977356\n",
      "0.33129873871803284\n",
      "0.33569595217704773\n",
      "0.38006529211997986\n",
      "Step 180/256 loss 3.8007e-01\n",
      "0.33683425188064575\n",
      "0.34499815106391907\n",
      "0.3337412476539612\n",
      "0.3640338182449341\n",
      "0.3486882746219635\n",
      "0.342973917722702\n",
      "0.34515586495399475\n",
      "0.3457143306732178\n",
      "0.37631502747535706\n",
      "0.36651289463043213\n",
      "Step 190/256 loss 3.6651e-01\n",
      "0.34163305163383484\n",
      "0.366092324256897\n",
      "0.346109539270401\n",
      "0.4122124910354614\n",
      "0.35676130652427673\n",
      "0.366926372051239\n",
      "0.3482908606529236\n",
      "0.3573378920555115\n",
      "0.3632434010505676\n",
      "0.33787253499031067\n",
      "Step 200/256 loss 3.3787e-01\n",
      "0.31868040561676025\n",
      "0.34022045135498047\n",
      "0.38073036074638367\n",
      "0.3538798391819\n",
      "0.3614855110645294\n",
      "0.3542250096797943\n",
      "0.3425997793674469\n",
      "0.3240013122558594\n",
      "0.30766111612319946\n",
      "0.34571027755737305\n",
      "Step 210/256 loss 3.4571e-01\n",
      "0.3426453471183777\n",
      "0.38610586524009705\n",
      "0.32988592982292175\n",
      "0.3822251856327057\n",
      "0.36296698451042175\n",
      "0.32338181138038635\n",
      "0.3008578419685364\n",
      "0.3629496693611145\n",
      "0.3234042227268219\n",
      "0.36410075426101685\n",
      "Step 220/256 loss 3.6410e-01\n",
      "0.33749061822891235\n",
      "0.3284270763397217\n",
      "0.3497188091278076\n",
      "0.35071155428886414\n",
      "0.32337528467178345\n",
      "0.31924116611480713\n",
      "0.3091749846935272\n",
      "0.3017697036266327\n",
      "0.33290794491767883\n",
      "0.3233755826950073\n",
      "Step 230/256 loss 3.2338e-01\n",
      "0.3119851350784302\n",
      "0.3189529478549957\n",
      "0.3164798617362976\n",
      "0.32092174887657166\n",
      "0.3253209590911865\n",
      "0.31254899501800537\n",
      "0.326639860868454\n",
      "0.32949844002723694\n",
      "0.32340139150619507\n",
      "0.3119330406188965\n",
      "Step 240/256 loss 3.1193e-01\n",
      "0.34503504633903503\n",
      "0.3021602928638458\n",
      "0.2943539023399353\n",
      "0.3222355842590332\n",
      "0.3046490550041199\n",
      "0.37886756658554077\n",
      "0.3283652663230896\n",
      "0.32501986622810364\n",
      "0.3157673478126526\n",
      "0.33590003848075867\n",
      "Step 250/256 loss 3.3590e-01\n",
      "0.3202163577079773\n",
      "0.347745418548584\n",
      "0.33932048082351685\n",
      "0.36657026410102844\n",
      "0.34533682465553284\n",
      "0.3545168936252594\n",
      "Step 256/256 loss 3.5452e-01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7974515ecf04528ba53b12a4df9f55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "val:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 | train 4.2279e-01 | val 3.2773e-01\n",
      "0.33980730175971985\n",
      "0.3233066201210022\n",
      "0.37494105100631714\n",
      "0.3467027246952057\n",
      "0.3858273923397064\n",
      "0.3278050124645233\n",
      "0.3410329222679138\n",
      "0.3648868203163147\n",
      "0.3474820554256439\n",
      "0.35033321380615234\n",
      "Step 10/256 loss 3.5033e-01\n",
      "0.3709387183189392\n",
      "0.40857401490211487\n",
      "0.3980324864387512\n",
      "0.37423452734947205\n",
      "0.3217097520828247\n",
      "0.36231693625450134\n",
      "0.29988712072372437\n",
      "0.29867926239967346\n",
      "0.31375664472579956\n",
      "0.32129424810409546\n",
      "Step 20/256 loss 3.2129e-01\n",
      "0.30631133913993835\n",
      "0.3427363932132721\n",
      "0.30730316042900085\n",
      "0.32660186290740967\n",
      "0.2981935739517212\n",
      "0.3120017945766449\n",
      "0.3180062174797058\n",
      "0.3048381209373474\n",
      "0.3234547972679138\n",
      "0.3025783896446228\n",
      "Step 30/256 loss 3.0258e-01\n",
      "0.3433068096637726\n",
      "0.33572763204574585\n",
      "0.3197738230228424\n",
      "0.32156506180763245\n",
      "0.330477237701416\n",
      "0.3255014717578888\n",
      "0.30031558871269226\n",
      "0.2952449321746826\n",
      "0.30420583486557007\n",
      "0.30148831009864807\n",
      "Step 40/256 loss 3.0149e-01\n",
      "0.2792803645133972\n",
      "0.31876087188720703\n",
      "0.27887845039367676\n",
      "0.28178131580352783\n",
      "0.29714998602867126\n",
      "0.28848394751548767\n",
      "0.299678236246109\n",
      "0.24817034602165222\n",
      "0.2416262924671173\n",
      "0.26395100355148315\n",
      "Step 50/256 loss 2.6395e-01\n",
      "0.26019489765167236\n",
      "0.2540278434753418\n",
      "0.23009473085403442\n",
      "0.2615625262260437\n",
      "0.24394908547401428\n",
      "0.2505534589290619\n",
      "0.23591819405555725\n",
      "0.2252679467201233\n",
      "0.24338790774345398\n",
      "0.24886079132556915\n",
      "Step 60/256 loss 2.4886e-01\n",
      "0.24087047576904297\n",
      "0.24220147728919983\n",
      "0.21716219186782837\n",
      "0.224546879529953\n",
      "0.2325259894132614\n",
      "0.24353916943073273\n",
      "0.23951472342014313\n",
      "0.21812933683395386\n",
      "0.2400837242603302\n",
      "0.22578410804271698\n",
      "Step 70/256 loss 2.2578e-01\n",
      "0.24709129333496094\n",
      "0.23863127827644348\n",
      "0.2238055020570755\n",
      "0.23458576202392578\n",
      "0.23641261458396912\n",
      "0.22963866591453552\n",
      "0.28689125180244446\n",
      "0.21507351100444794\n",
      "0.23612898588180542\n",
      "0.23175400495529175\n",
      "Step 80/256 loss 2.3175e-01\n",
      "0.2470412403345108\n",
      "0.2355344295501709\n",
      "0.22287118434906006\n",
      "0.22336982190608978\n",
      "0.2424784004688263\n",
      "0.2215036302804947\n",
      "0.22810769081115723\n",
      "0.2583121955394745\n",
      "0.23546409606933594\n",
      "0.21041911840438843\n",
      "Step 90/256 loss 2.1042e-01\n",
      "0.22716401517391205\n",
      "0.22886958718299866\n",
      "0.20864863693714142\n",
      "0.23600266873836517\n",
      "0.2307046800851822\n",
      "0.22762390971183777\n",
      "0.23416167497634888\n",
      "0.21796981990337372\n",
      "0.25974032282829285\n",
      "0.2062651813030243\n",
      "Step 100/256 loss 2.0627e-01\n",
      "0.2542223036289215\n",
      "0.22372451424598694\n",
      "0.2288253903388977\n",
      "0.22824382781982422\n",
      "0.2173372209072113\n",
      "0.21741481125354767\n",
      "0.21473237872123718\n",
      "0.24736936390399933\n",
      "0.22216582298278809\n",
      "0.21663488447666168\n",
      "Step 110/256 loss 2.1663e-01\n",
      "0.22753937542438507\n",
      "0.2187121957540512\n",
      "0.22770313918590546\n",
      "0.21294765174388885\n",
      "0.21480530500411987\n",
      "0.20526531338691711\n",
      "0.18844424188137054\n",
      "0.20771187543869019\n",
      "0.18912534415721893\n",
      "0.20415128767490387\n",
      "Step 120/256 loss 2.0415e-01\n",
      "0.18538875877857208\n",
      "0.20043715834617615\n",
      "0.2113787978887558\n",
      "0.1930616945028305\n",
      "0.23855029046535492\n",
      "0.1915271282196045\n",
      "0.17881202697753906\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for epoch in range(1, CONFIG['epochs'] + 1):\n",
    "    train_loss = train_epoch_debug(model, train_loader, optimizer, loss_fn, device, log_every=CONFIG['log_every'])\n",
    "    val_loss = test_loop(model, val_loader, loss_fn, device)\n",
    "    print(f\"Epoch {epoch}/{CONFIG['epochs']} | train {train_loss:.4e} | val {val_loss:.4e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58488050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.cx_unet import ComplexConv2d\n",
    "\n",
    "# Set up a toy complex conv with zero weights so only biases remain.\n",
    "conv = ComplexConv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1, bias=True).eval()\n",
    "with torch.no_grad():\n",
    "    conv.conv_real.weight.zero_()\n",
    "    conv.conv_imag.weight.zero_()\n",
    "    conv.conv_real.bias.fill_(1.0)    # pretend the real bias should be +1\n",
    "    conv.conv_imag.bias.fill_(0.25)   # pretend the imaginary bias should be +0.25\n",
    "\n",
    "# Feed an all-zero complex tensor; any non-zero output must come from the bias math.\n",
    "x = torch.zeros(1, 1, 8, 8, dtype=torch.complex64)\n",
    "y = conv(x)\n",
    "\n",
    "print(\"Output real part (should equal 1.0 if biasing were correct):\", y.real.unique())\n",
    "print(\"Output imag part (should equal 0.25 if biasing were correct):\", y.imag.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbbf7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab59d329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece570",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
