\section{Methodology}
\label{sec:methodology}
I keep the pipeline minimal but reproducible. The final configuration matches the overnight runs: \textbf{train} 16{,}384 slices, \textbf{val} 1{,}024 slices, equispaced masks at $R=6$ with 20 ACS lines, batch size 4, and roughly 50k update steps (12 epochs). Widths for the complex U-Net are $[16,32,64,128,256]$; the real U-Net uses the same widths with a $1.42\times$ scale to match the parameter count (about 7.8M per model).
\subsection{Dataset and Preprocessing}
The experiments use the fastMRI \cite{zbontar2019fastmriopendatasetbenchmarks} single-coil knee dataset. Each slice is masked with an equispaced pattern at $R=6$ and 20 ACS lines, inverse FFTs produce zero-filled inputs, and ground-truth targets are center-cropped to $640\times320$. Both masked inputs and targets are normalized by the per-slice maximum magnitude. No data augmentation is applied. The \texttt{debug\_trainer.ipynb} notebook mirrors this exact preprocessing for quick inspection.
\subsection{Baseline Masking}
Zero-filled reconstructions anchor the study. I generate reconstructions for $R \in \{2,4,6,8\}$ and ACS $\in \{16,20,24\}$, reporting PSNR/SSIM/$\ell_1$ in Table~\ref{tab:zerofill_metrics} and showing representative slices in Figure~\ref{fig:zerofill_grid}. These runs reuse the same loader and normalization as the trained models.
\subsection{Model Architectures}
Both models follow the U-Net macro-architecture from \citet{cole2020analysisdeepcomplexvaluedconvolutional}. The complex U-Net uses CReLU activations, complex batch normalization, and component-wise pooling; the real U-Net uses standard convolutions and ReLU. Feature widths are matched in parameter count by scaling the real channels by $1.42$. Skip connections and bottlenecks mirror each other so differences stem only from complex versus real arithmetic.
\subsection{Training and Evaluation Protocol}
I train with Adam (learning rate $1\times10^{-3}$, betas $(0.9, 0.999)$) for 12 epochs (about 50k steps) with batch size 4. The loss is $\ell_1$ between reconstructed and ground-truth complex images. After each epoch I log validation PSNR/SSIM/$\ell_1$ (Table~\ref{tab:main_results}) and save latest/best checkpoints; qualitative slices at a fixed validation index feed Figure~\ref{fig:qualitative}. Per-step losses (logged every step in the quick runs, every 100 steps in the long runs) support the training-curve plot in Figure~\ref{fig:training_curve}.
