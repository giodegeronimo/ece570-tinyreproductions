\section{Background and Related Work}
\subsection{MRI Reconstruction Primer}
    At a high level, MRI acquires data in the frequency domain (k-space) by measuring the response of hydrogen nuclei to magnetic fields. Specifically, it collects the complex-valued Fourier coefficients of the spatial image, which can then be transformed back into the image domain using an inverse Fast Fourier Transform (iFFT). In a perfect world, the k-space would be sampled at the Nyquist rate to ensure accurate reconstruction. However, this is often impractical due to time and resource constraints. Instead, undersampling strategies are employed to reduce acquisition time and costs at the expense of image quality. While there are various undersampling patterns (random, variable density, etc.), this work focuses on equispaced undersampling with an auto-calibration signal (ACS) region. This means the we sample every $R^{th}$ line in k-space, where $R$ is the acceleration factor, while preserving a small central block of low-frequency lines (the ACS) to aid in reconstruction. In general, ACS lines can be viewed as the crucial low-frequency information needed for reconstruction, as they capture the overall structure and contrast of the image. As such, they are sampled fully even when undersampling. As we increase the acceleration factor $R$, we sample fewer lines in k-space outside of the ACS region, leading to faster acquisitions but more challenging reconstruction for high frequency features such as edges and fine details. We aim to restore these details using deep learning models.
% \begin{itemize}
%     \item \textbf{Forward model recap.} MRI acquires Fourier coefficients in k-space; undersampling corresponds to element-wise masking of the fully sampled k-space tensor followed by an inverse FFT to get a zero-filled image.
%     \item \textbf{Role of ACS.} Auto-calibration signal (ACS) lines preserve the low-frequency content needed for sensitivity estimation or residual learning; we follow fastMRI convention with centered ACS of width 8--24 lines.
%     \item \textbf{Metrics we will report.} PSNR (signal fidelity), SSIM (structural similarity on magnitude images), and $\ell_1$ (perceptual sharpness proxy). These are computed after per-slice max normalization to mirror \citet{cole2020analysisdeepcomplexvaluedconvolutional}.
%     \item \textbf{Figure placeholder.} \textit{Figure~\ref{fig:pipeline_placeholder}} (to be added) will illustrate the mask $\rightarrow$ inverse FFT $\rightarrow$ reconstruction flow.
% \end{itemize}
\subsection{Complex-Valued Neural Networks}
    Complex-valued neural networks (CVNNs) extend traditional real-valued networks by working directly with complex numbers. At first glance, this may seem like a straightforward extension. However, CVNNs require careful consideration of operations such as convolution, activation functions, and normalization to ensure stable and effective training. This is mainly due to the fact that each parameter has two degrees of freedom (real and imaginary parts), and interactions between them can be non-trivial. Small adjustments in phase can compound throughout the network and lead to numerical instabilities if not handled properly; something I encountered in this project. While the field of CVNNs is fascinating, this section will focus primarily on the relevant components needed to reproduce \citet{cole2020analysisdeepcomplexvaluedconvolutional}'s complex U-Net. \cite{trabelsi2018deepcomplexnetworks} provide a comprehensive overview of CVNN modules, of which the following were implemented in this work.

    \subsubsection{Complex Convolutions}
    In real-valued CNNs, convolutional layers apply a set of learnable filters to the input data, producing feature maps that capture spatial information. In the spirit of abusing notation, we can denote this operation as $Y = W * X$ where $W$ is the filter tensor, $X$ is the input tensor, and $Y$ is the output tensor. In complex-valued CNNs, both the input and filters are complex-valued, meaning they have real and imaginary components. The complex convolution operation can be expressed as: $Y = W * X = (A + jB) * (X + jY) = (A * X - B * Y) + j(B * X + A * Y)$ where $W = A + jB$ and $X = X + jY$ \cite{trabelsi2018deepcomplexnetworks}. This effectively doubles the number of parameters compared to a real-valued convolution with the same kernel size and channel counts. 

    \subsubsection{Complex Activations}
    Activation functions introduce non-linearity into neural networks, enabling them to learn complex mappings. This is particularly challenging in CVNNs, as it can be shown that no non-linear function can be both complex-differentiable (holomorphic) and non-constant \cite{trabelsi2018deepcomplexnetworks}. Various strategies have been proposed to address this, such as applying real-valued activations separetely to the real and imaginary parts, or using magnitude-phase based activations. \cite{trabelsi2018deepcomplexnetworks} introduce a handful of complex activations, including CReLU (Complex ReLU), which can be formulated as $CReLU(z) = ReLU(Re(z)) + jReLU(Im(z))$. Another option is zReLU, which can be expressed as $zReLU(z) = z$ if $0 < arg(z) < \pi/2$ else $0$, effectively zeroing out any complex number not in the first quadrant. \cite{cole2020analysisdeepcomplexvaluedconvolutional} found that CReLU worked best for their complex U-Net, so I adopt it here.

    \subsubsection{Complex Batch Normalization}
    Batch Normalization, introduced by \cite{ioffe2015batchnormalizationacceleratingdeep} is a technique used to accelerate training and improve stability by normalizing layer inputs. It works by standardizing each channel to have zero mean and unit variance across the mini-batch, often followed by learnable scaling and shifting parameters. \cite{trabelsi2018deepcomplexnetworks} extend this concept to complex-valued inputs by treating each complex number as a 2D vector in the real-imaginary plane, and normalizing based on the covariance matrix of these vectors. This involves computing the mean vector and covariance matrix for each channel, then using these statistics to normalize the inputs. This is formulated as $\tilde{z} = V^{-\frac{1}{2}}(z-\mu)$ where $V$ is the covariance matrix and $\mu$ is the mean vector. This approach ensures that both the real and imaginary components are normalized in a way that preserves their joint distribution, which is crucial for maintaining the integrity of complex-valued features. As with classical batch norm, learnable scaling and shifting parameters $\gamma$ and $\beta$ are applied after normalization, where $\gamma=\begin{bmatrix}\gamma_{rr} & \gamma_{ri} \\ \gamma_{ir} & \gamma_{ii}\end{bmatrix}$ is a $2\times2$ matrix that can scale and rotate the normalized complex features, and $\beta$ is a complex bias term (in vector form). While \cite{cole2020analysisdeepcomplexvaluedconvolutional} does not explicitly state whether they used complex normalization, or which variant, I found that using complex batch norm from \cite{trabelsi2018deepcomplexnetworks} was crucial for training stability.

    \subsubsection{Initialization and Pooling Choices}
    To keep the variance of real and imaginary filters balanced, I use the complex Xavier-style initialization from \cite{trabelsi2018deepcomplexnetworks} (mirroring Glorot) for paired real/imaginary convolutional kernels; this matches the recommended practice in Cole et al.\ and avoids early-scale imbalances. The real-valued U-Net relies on PyTorch's default Kaiming initialization, which empirically keeps activation scales comparable. For downsampling, both networks use max-style pooling applied channelwise (component-wise pooling in the complex case) with $2\times2$ kernels and stride 2; this preserves the relative magnitudes of the real and imaginary components without introducing phase distortion that a magnitude-only pooling might cause.

% \begin{itemize}
%     \item \textbf{Complex convolutions.} Implemented as $W * x = (X * a - Y * b) + j(Y * a + X * b)$ where $a,b$ are real/imag parts; effectively doubles parameters compared to a real conv with the same channel counts.
%     \item \textbf{Activation + normalization.} C>ole et al. observed that simple CReLU activations and complex instance/batch norm are sufficient for U-Nets; we mirror their CReLU choice and rely on per-slice normalization in the dataset.
%     \item \textbf{Capacity matching.} Prior work matches the total trainable parameters when comparing complex vs. real nets by halving one networkâ€™s width; we keep this in mind when defining `features` and `width_scale` in the notebook.
%     \item \textbf{Debug considerations.} Complex nets are prone to numerical blow-ups if the learning rate is too high (see debug notebook results); hence we document the final LR used in Section~\ref{sec:method}.
% \end{itemize}
\subsection{Dataset: fastMRI Single-Coil Benchmarks}
    The fastMRI dataset, introduced by \cite{zbontar2019fastmriopendatasetbenchmarks} is a large-scale collection of MRI scans designed to facilitate research in accelerated MRI reconstruction. It includes both single-coil and multi-coil data, with various anatomies such as knee and brain scans. Crucially, the dataset provides fully sampled k-space data, allowing researchers to simulate undersampling and evaluate reconstruction algorithms. In this work, I focus on the single-coil knee subset, which consists of over 1,000 scans from different patients. Each scan features multiple slices, with each slice containing a 2D k-space tensor. These slices vary in spatial dimensions, so I center-crop them to a consistent size of $640\times320$ for training and evaluation. 
% \begin{itemize}
%     \item \textbf{Dataset quirks.} Single-coil HDF5 files contain complex-valued tensors with varying spatial sizes (e.g., $640\times368$); we center-crop to $640\times320$ for consistency.
%     \item \textbf{Baselines.} The fastMRI leaderboard reports zero-fill, classical compressed sensing, and real-valued U-Nets. Public complex-valued baselines are scarce, motivating this reproduction.
%     \item \textbf{Simplifications vs. Cole et al.} We use single-coil data, fewer epochs (15), and a single acceleration factor in the main comparison; this should be highlighted as a limitation later.
%     \item \textbf{Table placeholder.} \textit{Table~\ref{tab:dataset}} will summarize dataset splits, number of slices, and mask parameters used in our experiments.
% \end{itemize}
