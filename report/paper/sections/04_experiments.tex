\section{Experiments}
\label{sec:experiments}

% TODO(Experiments): Replace placeholder figure/table references once plots are generated.
% TODO(Experiments): Insert actual metric values for Table~\ref{tab:main_results} and Figure~\ref{fig:qualitative}.

I structure the experimental campaign around four questions that mirror the reproduction targets in Section~\ref{sec:targets}: (1) what is the quantitative gap between zero-filled reconstructions and a conventional real-valued U-Net on the fastMRI knee subset; (2) does a parameter-matched complex U-Net reduce that gap, as claimed by \citet{cole2020analysisdeepcomplexvaluedconvolutional}; (3) how sensitive is each model to the acceleration factor, and (4) when training destabilizes, what logging is needed to diagnose it.

\subsection{Runs and configurations}
\label{sec:exp-runs}
Each experiment reuses the training pipeline from Section~\ref{sec:methodology}. I seed the data loaders, model weights, and masking so that the real- and complex-valued models see the exact same batches. The following runs are scheduled:
\begin{itemize}
    \item \textbf{Zero-filled baseline.} No learning. I compute masked inverse FFTs for every validation slice to anchor Table~\ref{tab:main_results}. This also supplies the left panel of each qualitative grid.
    \item \textbf{Real U-Net.} The width configuration is identical to Section~\ref{sec:methodology}, yielding roughly $7.8$\,M parameters. This is my control reproduction of the ``two-channel'' approach reported in the original paper.
    \item \textbf{Complex U-Net.} I halve the width factors so that the total parameter count (real+imaginary) matches the real-valued model, keeping optimizer, schedule, and checkpoint cadence unchanged.
    \item \textbf{Stress tests / ablations.} When training diverges (observed in the debug notebook), I re-run with the same seed but different learning rates or gradient clipping thresholds. These runs will be summarized qualitatively in the Discussion.
\end{itemize}

\subsection{Metrics and logging}
\label{sec:exp-metrics}
For every checkpoint that saves to \texttt{results/}, my scripts record per-slice $\ell_1$ loss, PSNR, SSIM, and the normalized root-mean-square error (NRMSE) used by \citet{cole2020analysisdeepcomplexvaluedconvolutional}. Epoch-level aggregates are serialized to JSON for easy plotting, while the best and most recent checkpoints (plus qualitative reconstructions at a fixed validation index) are stored under \texttt{checkpoints/}. I will reference the quantitative summary in Table~\ref{tab:main_results} and qualitative grids in Figure~\ref{fig:qualitative}.

\subsection{Planned analyses}
\label{sec:exp-analysis-plan}
Once results are available, I will report:
\begin{enumerate}
    \item \textbf{Main comparison.} Side-by-side PSNR/SSIM/NRMSE means for zero-filled, real U-Net, and complex U-Net on the validation subset.
    \item \textbf{Training dynamics.} Learning curves from both models (Figure~\ref{fig:training_curve}) highlighting any instability in the complex-valued network.
    \item \textbf{Qualitative slices.} Consistent slice indices for all models so visual differences are attributable to the architecture rather than anatomy.
    \item \textbf{Runtime discussion.} Throughput numbers from \texttt{notebooks/unet\_speed\_benchmark.ipynb} to contextualize the 30$\times$ slowdown observed for complex convolutions.
\end{enumerate}
These analyses will feed directly into Sections~\ref{sec:results} and~\ref{sec:discussion}.
